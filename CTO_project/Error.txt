在运行bceloss = self.bce(pred, target)时遇到报错RuntimeError: CUDA error: device-side assert triggered
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


在网络训练过程中，只能训练一步，并得到“train: epoch 1, iter:0, loss: 1.6617, lr: 0.001”的正常结果，
但是第二个batch在输入到 vmamba_CTO.py
252 行 nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)，得到全为nan的输出


【可能的原因】
在神经网络训练过程中遇到输出全为NaN的情况通常是由于梯度爆炸或梯度消失导致的。这可能是由于多种原因引起的，例如参数初始化不当、学习率过大或过小、网络结构不合适等。以下是一些可能的解决方法：
参数初始化：确保权重和偏置参数的初始化在一个合理的范围内，避免出现梯度爆炸或梯度消失的情况。
学习率调整：尝试减小学习率，有时候学习率过大会导致梯度爆炸。可以使用学习率衰减策略，如学习率衰减或使用自适应学习率算法。
梯度裁剪：在训练过程中对梯度进行裁剪，以防止梯度爆炸。
BatchNormalization：确保网络中包含适当的批量归一化层，有助于稳定训练过程。
梯度检查：检查梯度是否存在异常值，可以通过打印梯度的范数或直接检查是否有NaN值来进行排查。
网络结构：检查网络结构是否存在梯度消失的情况，例如太深的网络可能会导致梯度消失问题。
数值稳定性：在计算过程中使用数值稳定的计算方法，避免出现数值溢出或下溢的情况。

【发现问题】
在engine.py中定义check_gradient函数来检查梯度是否存在异常值NaN


【猜测原因】
在生成边缘注意力图时(是基于x1和x4层特征的融合，得到的特征图数值近似，
有尝试使用nn.GELU, nn.SiLU等激活函数，但是激活作用有限，特征图数值差值还是仅有0.1之内，导致梯度回传时梯度消失)

【CTO模型可以借鉴的地方】在x,y两个方向上提取边缘特征
